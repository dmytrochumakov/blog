<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>IOS on Dmytro&#39;s Blog</title>
    <link>http://localhost:1313/tags/ios/</link>
    <description>Recent content in IOS on Dmytro&#39;s Blog</description>
    <image>
      <title>Dmytro&#39;s Blog</title>
      <url>http://localhost:1313/images/papermod-cover.png</url>
      <link>http://localhost:1313/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.123.3</generator>
    <language>en</language>
    <lastBuildDate>Sun, 02 Jun 2024 09:04:06 +0300</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/ios/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Accessibility iOS SwiftUI</title>
      <link>http://localhost:1313/posts/accessibility-ios-swiftui/</link>
      <pubDate>Sun, 02 Jun 2024 09:04:06 +0300</pubDate>
      <guid>http://localhost:1313/posts/accessibility-ios-swiftui/</guid>
      <description>Introduction Previously, I posted about Accessibility for UIKit. The idea behind this post is to find differences between UIKit Accessibility and SwiftUI features.
Similarities: Both UIKit and SwiftUI have accessibilityLabel and accessibilityHints APIs.
Differences: To use dynamic type for fonts, you need additional modifiers in SwiftUI. struct ScaledFont: ViewModifier { @Environment(\.sizeCategory) var sizeCategory var name: String var size: Double func body(content: Content) -&amp;gt; some View { let scaledSize = UIFontMetrics.default.scaledValue(for: size) return content.</description>
    </item>
    <item>
      <title>Accessibility iOS UIKit</title>
      <link>http://localhost:1313/posts/accessibility-ios-uikit/</link>
      <pubDate>Thu, 30 May 2024 07:04:49 +0300</pubDate>
      <guid>http://localhost:1313/posts/accessibility-ios-uikit/</guid>
      <description>Introduction I was curious to find out how to make an application more accessible. You can look at popular applications like YouTube or Netflix; they all have accessibility features like VoiceOver and dynamic fonts. I decided to create this example for a fruit calorie counter. It contains a list of fruits with the fruit name, fruit calories, and a favorite button.
Where to Start Before diving into implementation details, I want to highlight some information about the existing accessibility features and what I will be focusing on.</description>
    </item>
    <item>
      <title>Text To Speech iOS</title>
      <link>http://localhost:1313/posts/text-to-speech-ios/</link>
      <pubDate>Fri, 24 May 2024 07:52:07 +0300</pubDate>
      <guid>http://localhost:1313/posts/text-to-speech-ios/</guid>
      <description>Introduction I was eager to learn how converting Text To Speech works in iOS. Here is what I discovered:
First Step The first step is to add AVSpeechSynthesizer, an object that produces synthesized speech from text utterances.
@State private var speechSynthesizer = AVSpeechSynthesizer() Second Step The second step is to add AVSpeechUtterance, an object that encapsulates the text for speech synthesis.
private var utterance: AVSpeechUtterance { let inputMessage = &amp;#34;Hello world!</description>
    </item>
    <item>
      <title>Speech To Text iOS</title>
      <link>http://localhost:1313/posts/speech-to-text-ios/</link>
      <pubDate>Thu, 23 May 2024 08:37:45 +0300</pubDate>
      <guid>http://localhost:1313/posts/speech-to-text-ios/</guid>
      <description>Introduction I always wanted an iOS app that would allow me to economize my time by converting speech to text. I know this option is built into the keyboard, but you first need to click the text field, then tap on the microphone, and finally speak. I wanted a one-click option with the possibility to integrate it into all my daily routines. Here is what I discovered:
First Step The first step is to request authorization to access the device&amp;rsquo;s microphone using the Privacy - Speech Recognition Usage Description key and the Privacy - Microphone Usage Description key.</description>
    </item>
    <item>
      <title>Implementing GraphQL in an iOS application</title>
      <link>http://localhost:1313/posts/implementing-graphql-in-an-ios-application/</link>
      <pubDate>Wed, 15 May 2024 07:13:39 +0300</pubDate>
      <guid>http://localhost:1313/posts/implementing-graphql-in-an-ios-application/</guid>
      <description>Introduction I previously never had a chance to work with GraphQL. I was excited to learn when to apply this technology, what tools I can use, and how I can implement it. Here’s what I found:
For testing, I used the Star Wars GraphQL API with AllFilmsQuery:
query AllFilmsQuery { allFilms { films { title director created producers releaseDate } } } I requested allFilms with title, director, created, producers, and releaseDate information.</description>
    </item>
    <item>
      <title>Building Dynamic Island for Video Streaming App</title>
      <link>http://localhost:1313/posts/building-dynamic-island-for-video-streaming-app/</link>
      <pubDate>Mon, 06 May 2024 07:35:52 +0300</pubDate>
      <guid>http://localhost:1313/posts/building-dynamic-island-for-video-streaming-app/</guid>
      <description>Introduction I was curious about how to add Dynamic Island and implement it into a Video Streaming App. Here are a few steps on how you can achieve this.
Caveats Debugging Dynamic Island can be a bit tricky; it only works when the main app is running. If you try to run it separately, you will encounter the error SendProcessControlEvent:toPid: encountered an error: Error Domain=com.apple.dt.deviceprocesscontrolservice Code=8 &amp;quot;Failed to show Widget&amp;quot;. The solution is to configure live activities and run them through the main app.</description>
    </item>
    <item>
      <title>Building Video Streaming Widget for iOS App</title>
      <link>http://localhost:1313/posts/building-video-streaming-widget-for-ios-app/</link>
      <pubDate>Sun, 05 May 2024 07:20:29 +0300</pubDate>
      <guid>http://localhost:1313/posts/building-video-streaming-widget-for-ios-app/</guid>
      <description>Introduction I was exploring the idea of creating a YouTube-like widget for the lock screen on iOS devices. It wasn&amp;rsquo;t easy because most articles on the Internet discussed general implementations, such as for a coffee shop or a to-do list. Even when I found some similar versions, the project wouldn&amp;rsquo;t compile. I made the decision to approach it my way, so here&amp;rsquo;s what I found out:
Caveats After being stuck for two or more hours without understanding why, after tapping on a button, I wasn&#39;t able to receive a callback from it and the widget always opened the main iOS app, I realized that I forgot to add AppIntent - without it, you can&amp;rsquo;t handle actions for iOS 17.</description>
    </item>
    <item>
      <title>Building Video Streaming iOS App</title>
      <link>http://localhost:1313/posts/building-video-streaming-ios-app/</link>
      <pubDate>Fri, 03 May 2024 07:34:56 +0300</pubDate>
      <guid>http://localhost:1313/posts/building-video-streaming-ios-app/</guid>
      <description>Introduction I was looking for a way to add a video player to my iOS app that could be able to play remote videos.
Caveats Problem I found that you can&amp;rsquo;t open Vimeo or Youtube videos because of AVFoundationErrorDomain Code=-11850 &amp;quot;Operation Stopped&amp;quot; UserInfo={NSLocalizedFailureReason=The server is not correctly configured Domain=NSOSStatusErrorDomain Code=-12939 error. I don’t know exactly what this means, but I&amp;rsquo;m speculating it&amp;rsquo;s related to some protection.
Solution My solution was to find another video that is not related to those platforms.</description>
    </item>
    <item>
      <title>Building Group Chat using WebSockets</title>
      <link>http://localhost:1313/posts/building-group-chat-using-websockets/</link>
      <pubDate>Thu, 02 May 2024 15:15:14 +0300</pubDate>
      <guid>http://localhost:1313/posts/building-group-chat-using-websockets/</guid>
      <description>Introduction I never had a chance to work with WebSockets, so I decided to take a look and create a group chat. Here&amp;rsquo;s what I discovered:
To be able to send and receive messages, you need to create an interface for communication between a server and your application. In my case, I chose sendMessage and receiveMessage methods. For the server-side, I chose Node.js. For the iOS application, I chose the Socket.</description>
    </item>
    <item>
      <title>iOS AR App: Experience 3D Guitar</title>
      <link>http://localhost:1313/posts/ios-ar-app-experience-3d-guitar/</link>
      <pubDate>Wed, 01 May 2024 15:57:54 +0300</pubDate>
      <guid>http://localhost:1313/posts/ios-ar-app-experience-3d-guitar/</guid>
      <description>Introduction I was searching for an AR implementation of a 3D guitar inside an iOS app. Here&amp;rsquo;s what I discovered:
First Step The first step is not related to building the app. Before that you need to create a project using the Reality Composer Pro app (you can find it through Spotlight search).
Second Step After that, you need to visit https://developer.apple.com/augmented-reality/quick-look/ and download one of the USDZ files. In my case, I chose the 3D guitar.</description>
    </item>
  </channel>
</rss>
