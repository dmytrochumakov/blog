<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>IOS on Dmytro&#39;s Blog</title>
    <link>http://localhost:1313/tags/ios/</link>
    <description>Recent content in IOS on Dmytro&#39;s Blog</description>
    <image>
      <title>Dmytro&#39;s Blog</title>
      <url>http://localhost:1313/images/papermod-cover.png</url>
      <link>http://localhost:1313/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.123.3</generator>
    <language>en</language>
    <lastBuildDate>Sun, 07 Jul 2024 07:25:40 +0300</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/ios/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Battery Performance Testing for iOS App</title>
      <link>http://localhost:1313/posts/battery-performance-testing-for-ios-app/</link>
      <pubDate>Sun, 07 Jul 2024 07:25:40 +0300</pubDate>
      <guid>http://localhost:1313/posts/battery-performance-testing-for-ios-app/</guid>
      <description>Introduction Working with batteries on iOS devices for large applications has always been tricky. The amount of energy consumed by the screen, location services, network calls, processing, background tasks, etc., is significant. From a developer&amp;rsquo;s perspective, it seems complicated, but Xcode provides tools to address this problem.
To find the issue, you need to open Xcode and go to the Debug Navigator.
In the Debug Navigator, you will see the Energy Impact gauge.</description>
    </item>
    <item>
      <title>Creating a 2D Space Game for iOS Using SpriteKit - Part 2</title>
      <link>http://localhost:1313/posts/creating-2d-space-game-for-ios-using-spritekit-part-2/</link>
      <pubDate>Wed, 03 Jul 2024 06:39:04 +0300</pubDate>
      <guid>http://localhost:1313/posts/creating-2d-space-game-for-ios-using-spritekit-part-2/</guid>
      <description>Introduction In the previous chapter, I started talking about the video game creation process, from project setup to adding the background. Now, I&amp;rsquo;m going to add the player and physics to it.
You can download the project here.
First Step The first step is to initialize player using SKSpriteNode, set up player.position, and add player as a child node.
SKSpriteNode - is an onscreen graphical element that can be initialized from an image or a solid color.</description>
    </item>
    <item>
      <title>Creating a 2D Space Game for iOS Using SpriteKit - Part 1</title>
      <link>http://localhost:1313/posts/creating-2d-space-game-for-ios-using-spritekit-part-1/</link>
      <pubDate>Fri, 28 Jun 2024 07:18:59 +0300</pubDate>
      <guid>http://localhost:1313/posts/creating-2d-space-game-for-ios-using-spritekit-part-1/</guid>
      <description>Introduction I have never tried creating a game before; it feels like magic to me. I know that games have an enormous amount of underlying layers of abstractions and tools such as game engines, rendering, and so on. I have always been eager to learn at least 1% of the game creation process. In this article, I&amp;rsquo;m going to explore step-by-step instructions for creating a game for the iOS platform using SpriteKit.</description>
    </item>
    <item>
      <title>Testing push notifications locally in an iOS app</title>
      <link>http://localhost:1313/posts/testing-push-notifications-locally-in-an-ios-app/</link>
      <pubDate>Wed, 19 Jun 2024 07:13:19 +0300</pubDate>
      <guid>http://localhost:1313/posts/testing-push-notifications-locally-in-an-ios-app/</guid>
      <description>Introduction I always wondered how I could automate testing the push notification process. Even when Apple introduced the possibility of dragging a configured file to the simulator to display a notification, it is still a manual process. I&amp;rsquo;ll skip testing via the terminal because I think it takes more time than using an APNS file or the RocketSim app.
Before I was first introduced to the RocketSim app, I used an APNS file for testing push notifications.</description>
    </item>
    <item>
      <title>Adding Push Notifications to an iOS App</title>
      <link>http://localhost:1313/posts/adding-push-notifications-to-an-ios-app/</link>
      <pubDate>Fri, 14 Jun 2024 07:29:01 +0300</pubDate>
      <guid>http://localhost:1313/posts/adding-push-notifications-to-an-ios-app/</guid>
      <description>Introduction If you start a project from scratch, you need to always create some kind of service like PushNotificationService that will be responsible for handling push notification events. In this article, I want to explore a simple implementation of PushNotificationService to be able to reuse and customize it in future projects.
First Step The first step is to add the Push Notifications capability to your project. Go to your project -&amp;gt; Signing &amp;amp; Capabilities -&amp;gt; Tap + Capability -&amp;gt; Search for Push Notifications.</description>
    </item>
    <item>
      <title>Caching data using NSCache in iOS</title>
      <link>http://localhost:1313/posts/caching-data-using-nscache-in-ios/</link>
      <pubDate>Wed, 05 Jun 2024 07:13:58 +0300</pubDate>
      <guid>http://localhost:1313/posts/caching-data-using-nscache-in-ios/</guid>
      <description>Introduction I was curious about caching data using NSCache for an iOS app. So, I did some digging. Here is what I found:
Quick Overview NSCache helps store data in memory. When the application gets killed, it frees memory; it’s not persisted on disk. Storing data is carried out using a key-value pair mechanism like Dictionary. You can set automatic eviction to delete objects automatically. NSCache has multi-platform support: iOS, iPadOS, watchOS, macOS, and tvOS.</description>
    </item>
    <item>
      <title>Accessibility iOS SwiftUI</title>
      <link>http://localhost:1313/posts/accessibility-ios-swiftui/</link>
      <pubDate>Sun, 02 Jun 2024 09:04:06 +0300</pubDate>
      <guid>http://localhost:1313/posts/accessibility-ios-swiftui/</guid>
      <description>Introduction Previously, I posted about Accessibility for UIKit. The idea behind this post is to find differences between UIKit Accessibility and SwiftUI features.
Similarities: Both UIKit and SwiftUI have accessibilityLabel and accessibilityHints APIs.
Differences: To use dynamic type for fonts, you need additional modifiers in SwiftUI. struct ScaledFont: ViewModifier { @Environment(\.sizeCategory) var sizeCategory var name: String var size: Double func body(content: Content) -&amp;gt; some View { let scaledSize = UIFontMetrics.default.scaledValue(for: size) return content.</description>
    </item>
    <item>
      <title>Accessibility iOS UIKit</title>
      <link>http://localhost:1313/posts/accessibility-ios-uikit/</link>
      <pubDate>Thu, 30 May 2024 07:04:49 +0300</pubDate>
      <guid>http://localhost:1313/posts/accessibility-ios-uikit/</guid>
      <description>Introduction I was curious to find out how to make an application more accessible. You can look at popular applications like YouTube or Netflix; they all have accessibility features like VoiceOver and dynamic fonts. I decided to create this example for a fruit calorie counter. It contains a list of fruits with the fruit name, fruit calories, and a favorite button.
Where to Start Before diving into implementation details, I want to highlight some information about the existing accessibility features and what I will be focusing on.</description>
    </item>
    <item>
      <title>Text To Speech iOS</title>
      <link>http://localhost:1313/posts/text-to-speech-ios/</link>
      <pubDate>Fri, 24 May 2024 07:52:07 +0300</pubDate>
      <guid>http://localhost:1313/posts/text-to-speech-ios/</guid>
      <description>Introduction I was eager to learn how converting Text To Speech works in iOS. Here is what I discovered:
First Step The first step is to add AVSpeechSynthesizer, an object that produces synthesized speech from text utterances.
@State private var speechSynthesizer = AVSpeechSynthesizer() Second Step The second step is to add AVSpeechUtterance, an object that encapsulates the text for speech synthesis.
private var utterance: AVSpeechUtterance { let inputMessage = &amp;#34;Hello world!</description>
    </item>
    <item>
      <title>Speech To Text iOS</title>
      <link>http://localhost:1313/posts/speech-to-text-ios/</link>
      <pubDate>Thu, 23 May 2024 08:37:45 +0300</pubDate>
      <guid>http://localhost:1313/posts/speech-to-text-ios/</guid>
      <description>Introduction I always wanted an iOS app that would allow me to economize my time by converting speech to text. I know this option is built into the keyboard, but you first need to click the text field, then tap on the microphone, and finally speak. I wanted a one-click option with the possibility to integrate it into all my daily routines. Here is what I discovered:
First Step The first step is to request authorization to access the device&amp;rsquo;s microphone using the Privacy - Speech Recognition Usage Description key and the Privacy - Microphone Usage Description key.</description>
    </item>
    <item>
      <title>Implementing GraphQL in an iOS application</title>
      <link>http://localhost:1313/posts/implementing-graphql-in-an-ios-application/</link>
      <pubDate>Wed, 15 May 2024 07:13:39 +0300</pubDate>
      <guid>http://localhost:1313/posts/implementing-graphql-in-an-ios-application/</guid>
      <description>Introduction I previously never had a chance to work with GraphQL. I was excited to learn when to apply this technology, what tools I can use, and how I can implement it. Here’s what I found:
For testing, I used the Star Wars GraphQL API with AllFilmsQuery:
query AllFilmsQuery { allFilms { films { title director created producers releaseDate } } } I requested allFilms with title, director, created, producers, and releaseDate information.</description>
    </item>
    <item>
      <title>Building Dynamic Island for Video Streaming App</title>
      <link>http://localhost:1313/posts/building-dynamic-island-for-video-streaming-app/</link>
      <pubDate>Mon, 06 May 2024 07:35:52 +0300</pubDate>
      <guid>http://localhost:1313/posts/building-dynamic-island-for-video-streaming-app/</guid>
      <description>Introduction I was curious about how to add Dynamic Island and implement it into a Video Streaming App. Here are a few steps on how you can achieve this.
Caveats Debugging Dynamic Island can be a bit tricky; it only works when the main app is running. If you try to run it separately, you will encounter the error SendProcessControlEvent:toPid: encountered an error: Error Domain=com.apple.dt.deviceprocesscontrolservice Code=8 &amp;quot;Failed to show Widget&amp;quot;. The solution is to configure live activities and run them through the main app.</description>
    </item>
    <item>
      <title>Building Video Streaming Widget for iOS App</title>
      <link>http://localhost:1313/posts/building-video-streaming-widget-for-ios-app/</link>
      <pubDate>Sun, 05 May 2024 07:20:29 +0300</pubDate>
      <guid>http://localhost:1313/posts/building-video-streaming-widget-for-ios-app/</guid>
      <description>Introduction I was exploring the idea of creating a YouTube-like widget for the lock screen on iOS devices. It wasn&amp;rsquo;t easy because most articles on the Internet discussed general implementations, such as for a coffee shop or a to-do list. Even when I found some similar versions, the project wouldn&amp;rsquo;t compile. I made the decision to approach it my way, so here&amp;rsquo;s what I found out:
Caveats After being stuck for two or more hours without understanding why, after tapping on a button, I wasn&#39;t able to receive a callback from it and the widget always opened the main iOS app, I realized that I forgot to add AppIntent - without it, you can&amp;rsquo;t handle actions for iOS 17.</description>
    </item>
    <item>
      <title>Building Video Streaming iOS App</title>
      <link>http://localhost:1313/posts/building-video-streaming-ios-app/</link>
      <pubDate>Fri, 03 May 2024 07:34:56 +0300</pubDate>
      <guid>http://localhost:1313/posts/building-video-streaming-ios-app/</guid>
      <description>Introduction I was looking for a way to add a video player to my iOS app that could be able to play remote videos.
Caveats Problem I found that you can&amp;rsquo;t open Vimeo or Youtube videos because of AVFoundationErrorDomain Code=-11850 &amp;quot;Operation Stopped&amp;quot; UserInfo={NSLocalizedFailureReason=The server is not correctly configured Domain=NSOSStatusErrorDomain Code=-12939 error. I don’t know exactly what this means, but I&amp;rsquo;m speculating it&amp;rsquo;s related to some protection.
Solution My solution was to find another video that is not related to those platforms.</description>
    </item>
    <item>
      <title>Building Group Chat using WebSockets</title>
      <link>http://localhost:1313/posts/building-group-chat-using-websockets/</link>
      <pubDate>Thu, 02 May 2024 15:15:14 +0300</pubDate>
      <guid>http://localhost:1313/posts/building-group-chat-using-websockets/</guid>
      <description>Introduction I never had a chance to work with WebSockets, so I decided to take a look and create a group chat. Here&amp;rsquo;s what I discovered:
To be able to send and receive messages, you need to create an interface for communication between a server and your application. In my case, I chose sendMessage and receiveMessage methods. For the server-side, I chose Node.js. For the iOS application, I chose the Socket.</description>
    </item>
    <item>
      <title>iOS AR App: Experience 3D Guitar</title>
      <link>http://localhost:1313/posts/ios-ar-app-experience-3d-guitar/</link>
      <pubDate>Wed, 01 May 2024 15:57:54 +0300</pubDate>
      <guid>http://localhost:1313/posts/ios-ar-app-experience-3d-guitar/</guid>
      <description>Introduction I was searching for an AR implementation of a 3D guitar inside an iOS app. Here&amp;rsquo;s what I discovered:
First Step The first step is not related to building the app. Before that you need to create a project using the Reality Composer Pro app (you can find it through Spotlight search).
Second Step After that, you need to visit https://developer.apple.com/augmented-reality/quick-look/ and download one of the USDZ files. In my case, I chose the 3D guitar.</description>
    </item>
  </channel>
</rss>
